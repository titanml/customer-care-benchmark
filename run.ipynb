{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Benchmark Analysis of Takeoff for Consumer Complaints "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# read the data\n",
    "df = pd.read_csv(\"data/consumer_complaints.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input/Output Token analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "df['num_tokens'] = df['consumer_complaint'].apply(lambda x: len(tokenizer(x)['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare prompt input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Example prompt =====\n",
      " <s>[INST] You are a customer care specialist. Please read the customer complaint and categorize it. The categories you can chose from are - debt collection, mortgage, credit reporting, credit card, bank account or service, customer loan, and Other. Please only return the category classification.\n",
      "Customer Complaint: XXXX has claimed I owe them {$27.00} for XXXX years despite the PROOF of PAYMENT I sent them : canceled check and their ownPAID INVOICE for {$27.00}! \n",
      "They continue to insist I owe them and collection agencies are after me. \n",
      "How can I stop this harassment for a bill I already paid four years ago? [/INST]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"You are a customer care specialist. Please read the customer complaint and categorize it. The categories you can chose from are - debt collection, mortgage, credit reporting, credit card, bank account or service, customer loan, and Other. Please only return the category classification.\\nCustomer Complaint: \"\n",
    "\n",
    "# mistral chat template\n",
    "def chat_template(sys_prompt, user_input):\n",
    "    return \"<s>[INST] \" + sys_prompt + user_input +  \" [/INST]\"\n",
    "\n",
    "df['prompt'] = df['consumer_complaint'].apply(lambda x: chat_template(prompt, x))\n",
    "\n",
    "print(\"===== Example prompt =====\\n\", df['prompt'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_batch_10 = df['prompt'].head(10).to_list()\n",
    "prompt_batch_100 = df['prompt'].head(100).to_list()\n",
    "prompt_batch_1000 = df['prompt'].head(1000).to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper functions\n",
    "import time\n",
    "from takeoff_client import TakeoffClient\n",
    "\n",
    "def log_time_taken(start, end, num_prompts):\n",
    "    \"\"\"\n",
    "    Logs the time taken between start and end times in both seconds and a readable format.\n",
    "\n",
    "    Args:\n",
    "    start (float): The start time in seconds.\n",
    "    end (float): The end time in seconds.\n",
    "    \"\"\"\n",
    "    # Calculate the time difference\n",
    "    time_taken_seconds = end - start\n",
    "\n",
    "    # Convert seconds to a more readable format\n",
    "    hours = int(time_taken_seconds // 3600)\n",
    "    minutes = int((time_taken_seconds % 3600) // 60)\n",
    "    seconds = int(time_taken_seconds % 60)\n",
    "\n",
    "    # Log the time taken\n",
    "    print(f\"Time taken for {num_prompts} prompts: {time_taken_seconds:.2f} seconds => ({hours}h{minutes}m{seconds}s)\")\n",
    "\n",
    "\n",
    "def run_bench(num_rounds=2):\n",
    "    client = TakeoffClient(base_url=\"http://localhost\")\n",
    "    readers = client.get_readers()\n",
    "    model_name = readers['primary'][0]['model_name']\n",
    "    \n",
    "    print(\"====== Model Info ======\")\n",
    "    print(\"Model Name: \", model_name)\n",
    "    \n",
    "    for i in range(num_rounds):\n",
    "        print(\"======= Benchmark Round \", i+1, \" ========\")\n",
    "        \n",
    "        start = time.time()\n",
    "        response_10 = client.generate(prompt_batch_10)\n",
    "        end = time.time()\n",
    "        log_time_taken(start, end, 10)\n",
    "\n",
    "        start = time.time()\n",
    "        response_100 = client.generate(prompt_batch_100)\n",
    "        end = time.time()\n",
    "        log_time_taken(start, end, 100)\n",
    "        \n",
    "        start = time.time()\n",
    "        response_1000 = client.generate(prompt_batch_1000)\n",
    "        end = time.time()\n",
    "        log_time_taken(start, end, 1000)\n",
    "    \n",
    "    return response_10, response_100, response_1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting 1 Mistral Full\n",
    "\n",
    "- Model: mistralai/Mistral-7B-Instruct-v0.1\n",
    "- Server: takeoff v0.14.4\n",
    "- GPU: Nvidia A10G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Model Info ======\n",
      "Model Name:  mistralai/Mistral-7B-Instruct-v0.1\n",
      "======= Benchmark Round  1  ========\n",
      "Time taken for 10 prompts: 5.05 seconds => (0h0m5s)\n",
      "Time taken for 100 prompts: 28.18 seconds => (0h0m28s)\n",
      "Time taken for 1000 prompts: 257.29 seconds => (0h4m17s)\n",
      "======= Benchmark Round  2  ========\n",
      "Time taken for 10 prompts: 6.69 seconds => (0h0m6s)\n",
      "Time taken for 100 prompts: 31.00 seconds => (0h0m31s)\n",
      "Time taken for 1000 prompts: 253.57 seconds => (0h4m13s)\n"
     ]
    }
   ],
   "source": [
    "response_10, response_100, response_1000 = run_bench()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting 2 Mistral 4bit AWQ\n",
    "\n",
    "- Model: TheBloke/Mistral-7B-Instruct-v0.1-AWQ\n",
    "- Server: takeoff v0.14.4\n",
    "- GPU: Nvidia A10G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Model Info ======\n",
      "Model Name:  TheBloke/Mistral-7B-Instruct-v0.1-AWQ\n",
      "======= Benchmark Round  1  ========\n",
      "Time taken for 10 prompts: 4.81 seconds => (0h0m4s)\n",
      "Time taken for 100 prompts: 42.12 seconds => (0h0m42s)\n",
      "Time taken for 1000 prompts: 440.36 seconds => (0h7m20s)\n",
      "======= Benchmark Round  2  ========\n",
      "Time taken for 10 prompts: 10.83 seconds => (0h0m10s)\n",
      "Time taken for 100 prompts: 40.28 seconds => (0h0m40s)\n",
      "Time taken for 1000 prompts: 423.90 seconds => (0h7m3s)\n"
     ]
    }
   ],
   "source": [
    "response_10, response_100, response_1000 = run_bench()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting 3 Mixtral Full\n",
    "\n",
    "- Model: mistralai/Mixtral-8x7B-Instruct-v0.1\n",
    "- Server: takeoff v0.14.3\n",
    "- GPU: Nvidia A10G * 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Model Info ======\n",
      "Model Name:  mistralai/Mixtral-8x7B-Instruct-v0.1\n",
      "======= Benchmark Round  1  ========\n",
      "Time taken for 10 prompts: 19.50 seconds => (0h0m19s)\n",
      "Time taken for 100 prompts: 126.26 seconds => (0h2m6s)\n",
      "Time taken for 1000 prompts: 1366.86 seconds => (0h22m46s)\n",
      "======= Benchmark Round  2  ========\n",
      "Time taken for 10 prompts: 24.26 seconds => (0h0m24s)\n",
      "Time taken for 100 prompts: 133.32 seconds => (0h2m13s)\n",
      "Time taken for 1000 prompts: 1399.05 seconds => (0h23m19s)\n"
     ]
    }
   ],
   "source": [
    "response_10, response_100, response_1000 = run_bench()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting 4 Mixtral 4bit AWQ\n",
    "\n",
    "- Model: TheBloke/dolphin-2.7-mixtral-8x7b-AWQ\n",
    "- Server: takeoff v0.14.4\n",
    "- GPU: Nvidia A10G * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Model Info ======\n",
      "Model Name:  TheBloke/dolphin-2.7-mixtral-8x7b-AWQ\n",
      "======= Benchmark Round  1  ========\n",
      "Time taken for 10 prompts: 16.10 seconds => (0h0m16s)\n",
      "Time taken for 100 prompts: 180.82 seconds => (0h3m0s)\n",
      "Time taken for 1000 prompts: 1846.86 seconds => (0h30m46s)\n",
      "======= Benchmark Round  2  ========\n",
      "Time taken for 10 prompts: 22.87 seconds => (0h0m22s)\n",
      "Time taken for 100 prompts: 185.18 seconds => (0h3m5s)\n",
      "Time taken for 1000 prompts: 1854.00 seconds => (0h30m54s)\n"
     ]
    }
   ],
   "source": [
    "response_10, response_100, response_1000 = run_bench()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
